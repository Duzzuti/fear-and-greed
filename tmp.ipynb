{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the difference between the files in test_data and test_data3 that end in 2024-12-12 - 2024-12-19\n",
    "import pandas as pd\n",
    "import os\n",
    "dir_ = \"test_data/sp500/\"\n",
    "dir_3 = \"test_data3/sp500/\"\n",
    "count = 0\n",
    "\n",
    "file1_list = []\n",
    "file2_list = []\n",
    "\n",
    "df1_list = []\n",
    "df2_list = []\t\n",
    "\n",
    "for file in os.listdir(dir_):\n",
    "    if file.split(\"_\")[1] in [\"2024-12-12\", \"2024-12-13\"]:\n",
    "        file1_list.append(file)\n",
    "\n",
    "for file in os.listdir(dir_3):\n",
    "    if file.split(\"_\")[1] in [\"2024-12-18\", \"2024-12-19\"]:\n",
    "        file2_list.append(file)\n",
    "\n",
    "file1_list.sort(key=lambda x: x.split(\"_\")[2].split(\".csv\")[0])\n",
    "file2_list.sort(key=lambda x: x.split(\"_\")[2].split(\".csv\")[0])\n",
    "\n",
    "for file in file1_list:\n",
    "    file_df = pd.read_csv(os.path.join(dir_, file), index_col=0)\n",
    "    file_df = file_df[(file_df.index >= \"2024-01-01\") & (file_df.index <= \"2024-12-12\")]\n",
    "    df1_list.append(file_df)\n",
    "\n",
    "for file in file2_list:\n",
    "    file_df = pd.read_csv(os.path.join(dir_3, file), index_col=0)\n",
    "    file_df = file_df[(file_df.index >= \"2024-01-01\") & (file_df.index <= \"2024-12-12\")]\n",
    "    df2_list.append(file_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df1, df2 in zip(df1_list, df2_list):\n",
    "    print(df1[\"Adj Close\"].equals(df2[\"Adj Close\"]))\n",
    "    if not df1[\"Adj Close\"].equals(df2[\"Adj Close\"]):\n",
    "        # get the difference\n",
    "        for i in range(len(df1)):\n",
    "            if df1[\"Adj Close\"].iloc[i] != df2[\"Adj Close\"].iloc[i]:\n",
    "                print(df1[\"Adj Close\"].iloc[i], df2[\"Adj Close\"].iloc[i])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import utils\n",
    "\n",
    "df = utils.get_repo_data(\"sp500_companies.csv\")\n",
    "# get last row value\n",
    "last_value = df.iloc[-1][\"tickers\"]\n",
    "last_value.count(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.load_sp500_data(\"test_data2/\")\n",
    "#TODO load date count.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728\n"
     ]
    }
   ],
   "source": [
    "# check for empty files and add them to a replacement list\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "print(len(os.listdir(\"test_data/sp500\")))\n",
    "# iterate over all csv files in test_data/sp500 folder\n",
    "for file in os.listdir(\"test_data/sp500\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        # read csv file\n",
    "        df = pd.read_csv(f\"test_data/sp500/{file}\", header=[0,1], index_col=0)\n",
    "        # get ticker symbol from the header\n",
    "        ticker = df.columns.levels[1][0]\n",
    "        if df.empty:\n",
    "            # add ticker to replacement list\n",
    "            with open(\"replacement_list.csv\", \"a\") as f:\n",
    "                f.write(f\"{ticker},\\n\")\n",
    "            # remove file\n",
    "            os.remove(f\"test_data/sp500/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate stock price breadth\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# some date way in the past to load all data\n",
    "last_date = \"2024-01-01\"\n",
    "sp500_dir = \"test_data/sp500/\"\n",
    "\n",
    "# get last date of the data\n",
    "old_df = pd.read_csv(\"breadth_ratio_tmp.csv\", index_col=0)\n",
    "last_date = old_df.index[-1]\n",
    "date_count_data = pd.read_csv(\"date_count.csv\", index_col=0)\n",
    "#new_date = (pd.to_datetime(last_date).date() + pd.DateOffset(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "df_list = []\n",
    "# iterate over all csv files in test_data/sp500 folder\n",
    "for file in os.listdir(sp500_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        # read csv file\n",
    "        df = pd.read_csv(f\"{sp500_dir}{file}\", header=[0,1], index_col=0)\n",
    "        if df.index[-1] < last_date:\n",
    "            continue\n",
    "        file_start_date = file.split(\"_\")[0]\n",
    "        file_start_date = pd.to_datetime(file_start_date).date() + pd.DateOffset(days=365)\n",
    "        first_relevant_date = max(file_start_date.strftime(\"%Y-%m-%d\"), last_date)\n",
    "        # init date is the date that needs to be added to calculate the difference for the first date (the first date before the first relevant date in date_count_data)\n",
    "        init_date = date_count_data.index[(date_count_data.index < first_relevant_date)][-1]\n",
    "        # use date_count_data to add missing dates between the first relevant date and the last date\n",
    "        df = df.reindex(date_count_data.index[(date_count_data.index <= df.index[-1])], fill_value=None)\n",
    "        # fill any nans after the first valid value but before the last valid value\n",
    "        # Identify the indices to forward-fill\n",
    "        ffill_mask = df.notna() | df.bfill().notna()\n",
    "        # Forward-fill the data\n",
    "        df = df.ffill().where(ffill_mask)\n",
    "        # remove all irrelevant data\n",
    "        df = df[df.index >= init_date]\n",
    "        df_list.append(df)\n",
    "\n",
    "# Process each ticker\n",
    "for df in df_list:\n",
    "    # Calculate the daily price change\n",
    "    df['Change'] = df['Adj Close'].diff()\n",
    "    \n",
    "    # Define volumes for \"up\" and \"down\" days\n",
    "    df['Volume Up'] = df['Volume'].where(df['Change'] > 0, 0)  # Volume for \"up\" days\n",
    "    df['Volume Down'] = df['Volume'].where(df['Change'] < 0, 0)  # Volume for \"down\" days\n",
    "\n",
    "if not df_list:\n",
    "    print(\"No new data to process\")\n",
    "else:\n",
    "    # Combine all tickers' data\n",
    "    combined_breadth = pd.concat(df_list).groupby(level=0).sum()\n",
    "    # Calculate total breadth\n",
    "    res = combined_breadth['Volume Up'] / (combined_breadth['Volume Down'] + combined_breadth['Volume Up'])\n",
    "    res.fillna(0.5, inplace=True)\n",
    "    res = pd.DataFrame(res, columns=['Breadth Ratio'], index=combined_breadth.index)\n",
    "    res.to_csv(\"breadth_ratio_tmp4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot breadth ratio tmp2.csv and breadth_ratio csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "df = pd.read_csv(\"breadth_ratio_tmp2.csv\", index_col=0)\n",
    "df2 = pd.read_csv(\"repoData/breadth_ratio.csv\", index_col=0)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df2.index = pd.to_datetime(df2.index)\n",
    "plt.plot(df.index, df['Breadth Ratio'], label=\"tmp2\")\n",
    "plt.plot(df2.index, df2['Breadth Ratio'], label=\"repo\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot breadth_ratio and breadth_ratio2 in one graph\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import utils\n",
    "matplotlib.use('tkagg')\n",
    "\n",
    "df2 = pd.read_csv(\"breadth_ratio2.csv\", index_col=0)\n",
    "df2.index = pd.to_datetime(df2.index)\n",
    "# moving average\n",
    "#plt.plot(utils.normalize_tanh(df2.ewm(span=10).mean(), steepness=7, shift=-0.5), color='red')\n",
    "plt.plot(utils.normalize_tanh(df2.ewm(span=100, min_periods=50).mean() * 100, steepness=0.3, shift=-50), color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# df_list = []\n",
    "# # iterate over all csv files in test_data/sp500 folder\n",
    "# for file in os.listdir(\"test_data/sp500\"):\n",
    "#     if file.endswith(\".csv\"):\n",
    "#         # read csv file\n",
    "#         df = pd.read_csv(f\"test_data/sp500/{file}\", header=[0,1], index_col=0)\n",
    "#         file_start_date = file.split(\"_\")[0]\n",
    "#         file_start_date = pd.to_datetime(file_start_date).date() + pd.DateOffset(days=365)\n",
    "#         # remove all rows before 1 year from the start date\n",
    "#         df = df[df.index >= file_start_date.strftime(\"%Y-%m-%d\")]\n",
    "#         df_list.append(df)\n",
    "\n",
    "# # concat all dataframes while keeping nan values\n",
    "# combined_breadth = pd.concat(df_list, axis=1, join='outer')\n",
    "combined_breadth = pd.read_csv(\"combined_breadth.csv\", index_col=0, header=[0,1])\n",
    "data_adj_close = combined_breadth.xs('Adj Close', axis=1, level=0)\n",
    "# fill any nans after the first valid value but before the last valid value\n",
    "# Identify the indices to forward-fill\n",
    "ffill_mask = data_adj_close.notna() | data_adj_close.bfill().notna()\n",
    "# Forward-fill the data\n",
    "data_adj_close = data_adj_close.ffill().where(ffill_mask)\n",
    "# period is the number of days to look back for a 52 week high\n",
    "# 1. Calculate the 52 week highs and lows\n",
    "year_highs = data_adj_close.rolling(window=252, min_periods=1).max()\n",
    "year_lows = data_adj_close.rolling(window=252, min_periods=1).min()\n",
    "# 2. Calculate the high and low of the last period days\n",
    "period_highs = data_adj_close.rolling(window=10, min_periods=1).max()\n",
    "period_lows = data_adj_close.rolling(window=10, min_periods=1).min()\n",
    "# 3. Calculate the number of stocks that are at their 52 week high and low\n",
    "num_highs = ((data_adj_close == period_highs) & (data_adj_close == year_highs)).sum(axis=1)\n",
    "num_lows = ((data_adj_close == period_lows) & (data_adj_close == year_lows)).sum(axis=1)\n",
    "# 4. Calculate the stock price strength\n",
    "stock_price_strength = num_highs / (num_highs + num_lows) * 100\n",
    "# replace NaN values with 50 (no stocks at 52 week high or low)\n",
    "stock_price_strength.fillna(50, inplace=True)\n",
    "df = pd.DataFrame((num_highs, num_lows, stock_price_strength), index=[\"Num Highs\", \"Num Lows\", \"Stock Price Strength\"]).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Price Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import exit\n",
    "import pandas as pd\n",
    "import os\n",
    "import yfinance as yf\n",
    "# the number of days to look back for a high or low to count\n",
    "period = 14\n",
    "num_highs = {}\n",
    "num_lows = {}\n",
    "if not os.path.exists(\"test_data/trading_days.csv\"):\n",
    "    yf.download(\"^GSPC\", start=\"1995-01-01\").to_csv(\"test_data/trading_days.csv\")\n",
    "trading_days = pd.read_csv(\"test_data/trading_days.csv\", index_col=0, header=[0,1])\n",
    "trading_days.index = pd.to_datetime(trading_days.index)\n",
    "for date in trading_days.index:\n",
    "    num_highs[date] = 0\n",
    "    num_lows[date] = 0\n",
    "# iterate over all csv files in test_data/sp500 folder\n",
    "for file in os.listdir(\"test_data/sp500\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        # read csv file\n",
    "        df = pd.read_csv(f\"test_data/sp500/{file}\", header=[0,1], index_col=0, parse_dates=True)[\"Adj Close\"]\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        file_start_date = file.split(\"_\")[0]\n",
    "        actual_start_date = pd.to_datetime(file_start_date).date() + pd.DateOffset(days=365)\n",
    "        # add all trading days that are missing in the dataframe\n",
    "        df = df.reindex(trading_days.index, axis=0)\n",
    "        # Identify the indices to forward-fill\n",
    "        ffill_mask = df.notna() | df.bfill().notna()\n",
    "        # Forward-fill the data\n",
    "        df = df.ffill().where(ffill_mask)\n",
    "        df.dropna(inplace=True)\n",
    "        # get row that is after actual_start_date\n",
    "        first_valid_row = df[df.index >= actual_start_date].iloc[0]\n",
    "        year_high = df.loc[df[df.index <= first_valid_row.name].idxmax()].iloc[0]\n",
    "        year_low = df.loc[df[df.index <= first_valid_row.name].idxmin()].iloc[0]\n",
    "        current_row_index = df.index.get_loc(first_valid_row.name)\n",
    "        current_row = first_valid_row\n",
    "        while current_row_index < len(df) - 1 and current_row.notna().all():\n",
    "            if current_row.iloc[0] >= year_high.iloc[0]:\n",
    "                year_high = current_row\n",
    "            elif current_row.iloc[0] <= year_low.iloc[0]:\n",
    "                year_low = current_row\n",
    "            else:\n",
    "                one_year_ago = current_row.name - pd.DateOffset(days=365)\n",
    "                if year_high.name < one_year_ago:\n",
    "                    # calculate year_high again where the date is less than one year ago from the current date\n",
    "                    year_high = df.loc[df[(df.index >= one_year_ago) & (df.index <= current_row.name)].idxmax()].iloc[0]\n",
    "                if year_low.name < one_year_ago:\n",
    "                    # calculate year_low again\n",
    "                    year_low = df.loc[df[(df.index >= one_year_ago) & (df.index <= current_row.name)].idxmin()].iloc[0]\n",
    "            if current_row.name - pd.DateOffset(days=period) <= year_high.name:\n",
    "                num_highs[current_row.name] += 1\n",
    "            if current_row.name - pd.DateOffset(days=period) <= year_low.name:\n",
    "                num_lows[current_row.name] += 1\n",
    "            current_row_index += 1\n",
    "            current_row = df.iloc[current_row_index]\n",
    "        # TODO try percentage to low or high (e.g. (high - low)*0.2 distance (20 low, 70 high -> 20-30 low 60-70 high), 20 low 25 high > 20-21 low 24-25 high)\n",
    "        #compare with price_strength1.csv and price_strength2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import exit\n",
    "import pandas as pd\n",
    "import os\n",
    "import yfinance as yf\n",
    "num_highs = {}\n",
    "num_lows = {}\n",
    "if not os.path.exists(\"test_data/trading_days.csv\"):\n",
    "    yf.download(\"^GSPC\", start=\"1995-01-01\").to_csv(\"test_data/trading_days.csv\")\n",
    "trading_days = pd.read_csv(\"test_data/trading_days.csv\", index_col=0, header=[0,1])\n",
    "trading_days.index = pd.to_datetime(trading_days.index)\n",
    "for date in trading_days.index:\n",
    "    num_highs[date] = 0\n",
    "    num_lows[date] = 0\n",
    "# iterate over all csv files in test_data/sp500 folder\n",
    "for file in os.listdir(\"test_data/sp500\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        # read csv file\n",
    "        df = pd.read_csv(f\"test_data/sp500/{file}\", header=[0,1], index_col=0, parse_dates=True)[\"Adj Close\"]\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        file_start_date = file.split(\"_\")[0]\n",
    "        actual_start_date = pd.to_datetime(file_start_date).date() + pd.DateOffset(days=365)\n",
    "        # add all trading days that are missing in the dataframe\n",
    "        df = df.reindex(trading_days.index, axis=0)\n",
    "        # Identify the indices to forward-fill\n",
    "        ffill_mask = df.notna() | df.bfill().notna()\n",
    "        # Forward-fill the data\n",
    "        df = df.ffill().where(ffill_mask)\n",
    "        df.dropna(inplace=True)\n",
    "        # get row that is after actual_start_date\n",
    "        first_valid_row = df[df.index >= actual_start_date].iloc[0]\n",
    "        year_high = df.loc[df[df.index <= first_valid_row.name].idxmax()].iloc[0]\n",
    "        year_low = df.loc[df[df.index <= first_valid_row.name].idxmin()].iloc[0]\n",
    "        current_row_index = df.index.get_loc(first_valid_row.name)\n",
    "        current_row = first_valid_row\n",
    "        while current_row_index < len(df) - 1 and current_row.notna().all():\n",
    "            if current_row.iloc[0] >= year_high.iloc[0]:\n",
    "                year_high = current_row\n",
    "            elif current_row.iloc[0] <= year_low.iloc[0]:\n",
    "                year_low = current_row\n",
    "            else:\n",
    "                one_year_ago = current_row.name - pd.DateOffset(days=365)\n",
    "                if year_high.name < one_year_ago:\n",
    "                    # calculate year_high again where the date is less than one year ago from the current date\n",
    "                    year_high = df.loc[df[(df.index >= one_year_ago) & (df.index <= current_row.name)].idxmax()].iloc[0]\n",
    "                if year_low.name < one_year_ago:\n",
    "                    # calculate year_low again\n",
    "                    year_low = df.loc[df[(df.index >= one_year_ago) & (df.index <= current_row.name)].idxmin()].iloc[0]\n",
    "            if current_row.iloc[0] >= year_high.iloc[0] - (year_high.iloc[0] - year_low.iloc[0]) * 0.2:\n",
    "                num_highs[current_row.name] += 1\n",
    "            if current_row.iloc[0] <= year_low.iloc[0] + (year_high.iloc[0] - year_low.iloc[0]) * 0.2:\n",
    "                num_lows[current_row.name] += 1\n",
    "            current_row_index += 1\n",
    "            current_row = df.iloc[current_row_index]\n",
    "        # TODO try percentage to low or high (e.g. (high - low)*0.2 distance (20 low, 70 high -> 20-30 low 60-70 high), 20 low 25 high > 20-21 low 24-25 high)\n",
    "        #compare with price_strength1.csv and price_strength2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame((num_highs, num_lows), index=[\"Num Highs\", \"Num Lows\"]).T\n",
    "df[\"Strength\"] = df[\"Num Highs\"] / (df[\"Num Highs\"] + df[\"Num Lows\"]) * 100\n",
    "df.to_csv(\"price_strength4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Price Breadth\n",
    "\n",
    "1. do not count unchanged price volumes\n",
    "2. test if pd.concat works correctly (missing values, ranges, zeros, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
